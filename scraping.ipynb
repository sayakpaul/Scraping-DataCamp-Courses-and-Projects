{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic web-scrapping practice with `BeautifulSoup`. I was curious to gather all the names of DataCamp courses that are live as of now. This is just a starter notebook. There are lots of redundant codes as well which I will remove as I proceed. I will work more on it to improve upon my scrapping skills. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_by_technologies = ['tech:r', 'tech:python', 'tech:sql', 'tech:git', 'tech:shell', 'tech:spreadsheets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_url = 'https://www.datacamp.com/courses/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping https://www.datacamp.com/courses/tech:r\n",
      "Scrapping https://www.datacamp.com/courses/tech:python\n",
      "Scrapping https://www.datacamp.com/courses/tech:sql\n",
      "Scrapping https://www.datacamp.com/courses/tech:git\n",
      "Scrapping https://www.datacamp.com/courses/tech:spreadsheets\n",
      "\n",
      "\n",
      "Scrapping done!\n",
      "138\n",
      "63\n",
      "3\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "all_r_courses = []\n",
    "all_python_courses = []\n",
    "all_sql_courses = []\n",
    "all_git_courses = []\n",
    "all_spreadsheets_courses = []\n",
    "\n",
    "for topic in topic_by_technologies:\n",
    "    if topic == 'tech:r':\n",
    "        url_to_be_scrapped = root_url + topic\n",
    "        print('Scrapping ' + url_to_be_scrapped)\n",
    "        page_content_per_topic = requests.get(url_to_be_scrapped, headers=headers)\n",
    "        soup = BeautifulSoup(page_content_per_topic.text, 'html.parser')\n",
    "        res = soup.find_all('div', attrs={'class': 'courses__explore-list row'})\n",
    "        for entry in res:\n",
    "            urls = entry.find_all('h4', attrs={'class': 'course-block__title'})\n",
    "            for url in urls:\n",
    "                all_r_courses.append(url.get_text())\n",
    "    if topic == 'tech:python':\n",
    "        url_to_be_scrapped = root_url + topic\n",
    "        print('Scrapping ' + url_to_be_scrapped)\n",
    "        page_content_per_topic = requests.get(url_to_be_scrapped, headers=headers)\n",
    "        soup = BeautifulSoup(page_content_per_topic.text, 'html.parser')\n",
    "        res = soup.find_all('div', attrs={'class': 'courses__explore-list row'})\n",
    "        for entry in res:\n",
    "            urls = entry.find_all('h4', attrs={'class': 'course-block__title'})\n",
    "            for url in urls:\n",
    "                all_python_courses.append(url.get_text())\n",
    "    if topic == 'tech:sql':\n",
    "        url_to_be_scrapped = root_url + topic\n",
    "        print('Scrapping ' + url_to_be_scrapped)\n",
    "        page_content_per_topic = requests.get(url_to_be_scrapped, headers=headers)\n",
    "        soup = BeautifulSoup(page_content_per_topic.text, 'html.parser')\n",
    "        res = soup.find_all('div', attrs={'class': 'courses__explore-list row'})\n",
    "        for entry in res:\n",
    "            urls = entry.find_all('h4', attrs={'class': 'course-block__title'})\n",
    "            for url in urls:\n",
    "                all_sql_courses.append(url.get_text())\n",
    "    if topic == 'tech:git':\n",
    "        url_to_be_scrapped = root_url + topic\n",
    "        print('Scrapping ' + url_to_be_scrapped)\n",
    "        page_content_per_topic = requests.get(url_to_be_scrapped, headers=headers)\n",
    "        soup = BeautifulSoup(page_content_per_topic.text, 'html.parser')\n",
    "        res = soup.find_all('div', attrs={'class': 'courses__explore-list row'})\n",
    "        for entry in res:\n",
    "            urls = entry.find_all('h4', attrs={'class': 'course-block__title'})\n",
    "            for url in urls:\n",
    "                all_git_courses.append(url.get_text())\n",
    "    if topic == 'tech:spreadsheets':\n",
    "        url_to_be_scrapped = root_url + topic\n",
    "        print('Scrapping ' + url_to_be_scrapped)\n",
    "        page_content_per_topic = requests.get(url_to_be_scrapped, headers=headers)\n",
    "        soup = BeautifulSoup(page_content_per_topic.text, 'html.parser')\n",
    "        res = soup.find_all('div', attrs={'class': 'courses__explore-list row'})\n",
    "        for entry in res:\n",
    "            urls = entry.find_all('h4', attrs={'class': 'course-block__title'})\n",
    "            for url in urls:\n",
    "                all_spreadsheets_courses.append(url.get_text())\n",
    "print('\\n')\n",
    "print('Scrapping done!')\n",
    "print(len(all_r_courses))\n",
    "print(len(all_python_courses))\n",
    "print(len(all_sql_courses))\n",
    "print(len(all_git_courses))\n",
    "print(len(all_spreadsheets_courses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_courses = pd.DataFrame(data = all_r_courses, columns = ['Course Name'])\n",
    "r_courses['Topic Name'] = 'R'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_courses = pd.DataFrame(data = all_python_courses, columns = ['Course Name'])\n",
    "python_courses['Topic Name'] = 'Python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_courses = pd.DataFrame(data = all_sql_courses, columns = ['Course Name'])\n",
    "sql_courses['Topic Name'] = 'SQL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_courses = pd.DataFrame(data = all_git_courses, columns = ['Course Name'])\n",
    "git_courses['Topic Name'] = 'Git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet_courses = pd.DataFrame(data = all_spreadsheets_courses, columns = ['Course Name'])\n",
    "spreadsheet_courses['Topic Name'] = 'Spreadsheets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_courses = pd.concat([r_courses,python_courses,sql_courses,git_courses,spreadsheet_courses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved!\n"
     ]
    }
   ],
   "source": [
    "filename = 'DataCamp Courses as of ' + dt.now().strftime('%Y-%m-%d') + '.xlsx'\n",
    "writer = pd.ExcelWriter(filename)\n",
    "all_courses.to_excel(writer,'Sheet1',index=False)\n",
    "writer.save()\n",
    "print('File saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
